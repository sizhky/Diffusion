{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> Dev Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# | hide\\n# | hide\\n%reload_ext autoreload\\n%reload_ext nb_black\\n%autoreload 2\\nfrom nbdev.showdoc import *\\nimport sys\\n\\n__root = \\\"../\\\"\\nsys.path.append(__root)\";\n",
       "                var nbb_formatted_code = \"# | hide\\n# | hide\\n%reload_ext autoreload\\n%reload_ext nb_black\\n%autoreload 2\\nfrom nbdev.showdoc import *\\nimport sys\\n\\n__root = \\\"../\\\"\\nsys.path.append(__root)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "# | hide\n",
    "%reload_ext autoreload\n",
    "%reload_ext nb_black\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n",
    "import sys\n",
    "\n",
    "__root = \"../\"\n",
    "sys.path.append(__root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# | export\\nfrom torch_snippets import *\\nfrom torchvision.utils import save_image, make_grid\\nfrom matplotlib.animation import FuncAnimation, PillowWriter\";\n",
       "                var nbb_formatted_code = \"# | export\\nfrom torch_snippets import *\\nfrom torchvision.utils import save_image, make_grid\\nfrom matplotlib.animation import FuncAnimation, PillowWriter\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "from torch_snippets import *\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# | export\\nclass ResidualConvBlock(nn.Module):\\n    def __init__(\\n        self, in_channels: int, out_channels: int, is_res: bool = False\\n    ) -> None:\\n        super().__init__()\\n        self.same_channels = in_channels == out_channels\\n        self.is_res = is_res\\n        self.conv1 = nn.Sequential(\\n            nn.Conv2d(\\n                in_channels, out_channels, 3, 1, 1\\n            ),  # 3x3 kernel with stride 1 and padding 1\\n            nn.BatchNorm2d(out_channels),  # Batch normalization\\n            nn.GELU(),  # GELU activation function\\n        )\\n        self.conv2 = nn.Sequential(\\n            nn.Conv2d(\\n                out_channels, out_channels, 3, 1, 1\\n            ),  # 3x3 kernel with stride 1 and padding 1\\n            nn.BatchNorm2d(out_channels),  # Batch normalization\\n            nn.GELU(),  # GELU activation function\\n        )\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        if self.is_res:\\n            x1 = self.conv1(x)\\n            x2 = self.conv2(x1)\\n            if self.same_channels:\\n                out = x + x2\\n            else:\\n                shortcut = nn.Conv2d(\\n                    x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0\\n                ).to(x.device)\\n                out = shortcut(x) + x2\\n            return out / 1.414\\n        else:\\n            x1 = self.conv1(x)\\n            x2 = self.conv2(x1)\\n            return x2\\n\\n    def get_out_channels(self):\\n        return self.conv2[0].out_channels\\n\\n    def set_out_channels(self, out_channels):\\n        self.conv1[0].out_channels = out_channels\\n        self.conv2[0].in_channels = out_channels\\n        self.conv2[0].out_channels = out_channels\\n\\n\\nclass UnetUp(nn.Module):\\n    def __init__(self, in_channels, out_channels):\\n        super(UnetUp, self).__init__()\\n        layers = [\\n            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\\n            ResidualConvBlock(out_channels, out_channels),\\n            ResidualConvBlock(out_channels, out_channels),\\n        ]\\n\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x, skip):\\n        x = torch.cat((x, skip), 1)\\n        x = self.model(x)\\n        return x\\n\\n\\nclass UnetDown(nn.Module):\\n    def __init__(self, in_channels, out_channels):\\n        super(UnetDown, self).__init__()\\n        layers = [\\n            ResidualConvBlock(in_channels, out_channels),\\n            ResidualConvBlock(out_channels, out_channels),\\n            nn.MaxPool2d(2),\\n        ]\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        return self.model(x)\\n\\n\\nclass EmbedFC(nn.Module):\\n    def __init__(self, input_dim, emb_dim):\\n        super(EmbedFC, self).__init__()\\n        \\\"\\\"\\\"This class defines a generic one layer feed-forward neural network for embedding input data of\\n        dimensionality input_dim to an embedding space of dimensionality emb_dim.\\n        \\\"\\\"\\\"\\n        self.input_dim = input_dim\\n        layers = [\\n            nn.Linear(input_dim, emb_dim),\\n            nn.GELU(),\\n            nn.Linear(emb_dim, emb_dim),\\n        ]\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        x = x.view(-1, self.input_dim)\\n        return self.model(x)\\n\\n\\ndef unorm(x):\\n    \\\"\\\"\\\"unity norm. results in range of [0,1]\\n    assumes x has shape (h,w,3)\\n    \\\"\\\"\\\"\\n    xmax = x.max((0, 1))\\n    xmin = x.min((0, 1))\\n    return (x - xmin) / (xmax - xmin)\\n\\n\\ndef norm_all(store, n_t, n_s):\\n    \\\"\\\"\\\"runs unity norm on all timesteps of all samples\\\"\\\"\\\"\\n    nstore = np.zeros_like(store)\\n    for t in range(n_t):\\n        for s in range(n_s):\\n            nstore[t, s] = unorm(store[t, s])\\n    return nstore\\n\\n\\ndef norm_torch(x_all):\\n    \\\"\\\"\\\"runs unity norm on all timesteps of all samples\\n    input is assumed to be in (bs,3,h,w), the torch image format\\\"\\\"\\\"\\n    x = x_all.cpu().numpy()\\n    xmax = x.max((2, 3))\\n    xmin = x.min((2, 3))\\n    xmax = np.expand_dims(xmax, (2, 3))\\n    xmin = np.expand_dims(xmin, (2, 3))\\n    nstore = (x - xmin) / (xmax - xmin)\\n    return torch.from_numpy(nstore)\\n\\n\\ndef gen_tst_context(n_cfeat):\\n    \\\"\\\"\\\"\\n    Generate test context vectors\\n    \\\"\\\"\\\"\\n    vec = torch.tensor(\\n        [\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n        ]\\n    )\\n    return len(vec), vec\\n\\n\\ndef plot_grid(x, n_sample, n_rows, save_dir, w):\\n    \\\"\\\"\\\"x: (n_sample, 3, h, w)\\\"\\\"\\\"\\n    ncols = n_sample // n_rows\\n    grid = make_grid(\\n        norm_torch(x), nrow=ncols\\n    )  # curiously, nrow is number of columns.. or number of items in the row.\\n    save_image(grid, save_dir + f\\\"run_image_w{w}.png\\\")\\n    print(\\\"saved image at \\\" + save_dir + f\\\"run_image_w{w}.png\\\")\\n    return grid\\n\\n\\ndef plot_sample(x_gen_store, n_sample, nrows, save_dir, fn, w, save=False):\\n    ncols = n_sample // nrows\\n    sx_gen_store = np.moveaxis(\\n        x_gen_store, 2, 4\\n    )  # change to Numpy image format (h,w,channels) vs (channels,h,w)\\n    nsx_gen_store = norm_all(\\n        sx_gen_store, sx_gen_store.shape[0], n_sample\\n    )  # unity norm to put in range [0,1] for np.imshow\\n\\n    # create gif of images evolving over time, based on x_gen_store\\n    fig, axs = plt.subplots(\\n        nrows=nrows, ncols=ncols, sharex=True, sharey=True, figsize=(ncols, nrows)\\n    )\\n\\n    def animate_diff(i, store):\\n        print(f\\\"gif animating frame {i} of {store.shape[0]}\\\", end=\\\"\\\\r\\\")\\n        plots = []\\n        for row in range(nrows):\\n            for col in range(ncols):\\n                axs[row, col].clear()\\n                axs[row, col].set_xticks([])\\n                axs[row, col].set_yticks([])\\n                plots.append(axs[row, col].imshow(store[i, (row * ncols) + col]))\\n        return plots\\n\\n    ani = FuncAnimation(\\n        fig,\\n        animate_diff,\\n        fargs=[nsx_gen_store],\\n        interval=200,\\n        blit=False,\\n        repeat=True,\\n        frames=nsx_gen_store.shape[0],\\n    )\\n    plt.close()\\n    if save:\\n        ani.save(save_dir + f\\\"{fn}_w{w}.gif\\\", dpi=100, writer=PillowWriter(fps=5))\\n        print(\\\"saved gif at \\\" + save_dir + f\\\"{fn}_w{w}.gif\\\")\\n    return ani\";\n",
       "                var nbb_formatted_code = \"# | export\\nclass ResidualConvBlock(nn.Module):\\n    def __init__(\\n        self, in_channels: int, out_channels: int, is_res: bool = False\\n    ) -> None:\\n        super().__init__()\\n        self.same_channels = in_channels == out_channels\\n        self.is_res = is_res\\n        self.conv1 = nn.Sequential(\\n            nn.Conv2d(\\n                in_channels, out_channels, 3, 1, 1\\n            ),  # 3x3 kernel with stride 1 and padding 1\\n            nn.BatchNorm2d(out_channels),  # Batch normalization\\n            nn.GELU(),  # GELU activation function\\n        )\\n        self.conv2 = nn.Sequential(\\n            nn.Conv2d(\\n                out_channels, out_channels, 3, 1, 1\\n            ),  # 3x3 kernel with stride 1 and padding 1\\n            nn.BatchNorm2d(out_channels),  # Batch normalization\\n            nn.GELU(),  # GELU activation function\\n        )\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        if self.is_res:\\n            x1 = self.conv1(x)\\n            x2 = self.conv2(x1)\\n            if self.same_channels:\\n                out = x + x2\\n            else:\\n                shortcut = nn.Conv2d(\\n                    x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0\\n                ).to(x.device)\\n                out = shortcut(x) + x2\\n            return out / 1.414\\n        else:\\n            x1 = self.conv1(x)\\n            x2 = self.conv2(x1)\\n            return x2\\n\\n    def get_out_channels(self):\\n        return self.conv2[0].out_channels\\n\\n    def set_out_channels(self, out_channels):\\n        self.conv1[0].out_channels = out_channels\\n        self.conv2[0].in_channels = out_channels\\n        self.conv2[0].out_channels = out_channels\\n\\n\\nclass UnetUp(nn.Module):\\n    def __init__(self, in_channels, out_channels):\\n        super(UnetUp, self).__init__()\\n        layers = [\\n            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\\n            ResidualConvBlock(out_channels, out_channels),\\n            ResidualConvBlock(out_channels, out_channels),\\n        ]\\n\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x, skip):\\n        x = torch.cat((x, skip), 1)\\n        x = self.model(x)\\n        return x\\n\\n\\nclass UnetDown(nn.Module):\\n    def __init__(self, in_channels, out_channels):\\n        super(UnetDown, self).__init__()\\n        layers = [\\n            ResidualConvBlock(in_channels, out_channels),\\n            ResidualConvBlock(out_channels, out_channels),\\n            nn.MaxPool2d(2),\\n        ]\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        return self.model(x)\\n\\n\\nclass EmbedFC(nn.Module):\\n    def __init__(self, input_dim, emb_dim):\\n        super(EmbedFC, self).__init__()\\n        \\\"\\\"\\\"This class defines a generic one layer feed-forward neural network for embedding input data of\\n        dimensionality input_dim to an embedding space of dimensionality emb_dim.\\n        \\\"\\\"\\\"\\n        self.input_dim = input_dim\\n        layers = [\\n            nn.Linear(input_dim, emb_dim),\\n            nn.GELU(),\\n            nn.Linear(emb_dim, emb_dim),\\n        ]\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        x = x.view(-1, self.input_dim)\\n        return self.model(x)\\n\\n\\ndef unorm(x):\\n    \\\"\\\"\\\"unity norm. results in range of [0,1]\\n    assumes x has shape (h,w,3)\\n    \\\"\\\"\\\"\\n    xmax = x.max((0, 1))\\n    xmin = x.min((0, 1))\\n    return (x - xmin) / (xmax - xmin)\\n\\n\\ndef norm_all(store, n_t, n_s):\\n    \\\"\\\"\\\"runs unity norm on all timesteps of all samples\\\"\\\"\\\"\\n    nstore = np.zeros_like(store)\\n    for t in range(n_t):\\n        for s in range(n_s):\\n            nstore[t, s] = unorm(store[t, s])\\n    return nstore\\n\\n\\ndef norm_torch(x_all):\\n    \\\"\\\"\\\"runs unity norm on all timesteps of all samples\\n    input is assumed to be in (bs,3,h,w), the torch image format\\\"\\\"\\\"\\n    x = x_all.cpu().numpy()\\n    xmax = x.max((2, 3))\\n    xmin = x.min((2, 3))\\n    xmax = np.expand_dims(xmax, (2, 3))\\n    xmin = np.expand_dims(xmin, (2, 3))\\n    nstore = (x - xmin) / (xmax - xmin)\\n    return torch.from_numpy(nstore)\\n\\n\\ndef gen_tst_context(n_cfeat):\\n    \\\"\\\"\\\"\\n    Generate test context vectors\\n    \\\"\\\"\\\"\\n    vec = torch.tensor(\\n        [\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [0, 0, 0, 1, 0],\\n            [0, 0, 0, 0, 1],\\n            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\\n        ]\\n    )\\n    return len(vec), vec\\n\\n\\ndef plot_grid(x, n_sample, n_rows, save_dir, w):\\n    \\\"\\\"\\\"x: (n_sample, 3, h, w)\\\"\\\"\\\"\\n    ncols = n_sample // n_rows\\n    grid = make_grid(\\n        norm_torch(x), nrow=ncols\\n    )  # curiously, nrow is number of columns.. or number of items in the row.\\n    save_image(grid, save_dir + f\\\"run_image_w{w}.png\\\")\\n    print(\\\"saved image at \\\" + save_dir + f\\\"run_image_w{w}.png\\\")\\n    return grid\\n\\n\\ndef plot_sample(x_gen_store, n_sample, nrows, save_dir, fn, w, save=False):\\n    ncols = n_sample // nrows\\n    sx_gen_store = np.moveaxis(\\n        x_gen_store, 2, 4\\n    )  # change to Numpy image format (h,w,channels) vs (channels,h,w)\\n    nsx_gen_store = norm_all(\\n        sx_gen_store, sx_gen_store.shape[0], n_sample\\n    )  # unity norm to put in range [0,1] for np.imshow\\n\\n    # create gif of images evolving over time, based on x_gen_store\\n    fig, axs = plt.subplots(\\n        nrows=nrows, ncols=ncols, sharex=True, sharey=True, figsize=(ncols, nrows)\\n    )\\n\\n    def animate_diff(i, store):\\n        print(f\\\"gif animating frame {i} of {store.shape[0]}\\\", end=\\\"\\\\r\\\")\\n        plots = []\\n        for row in range(nrows):\\n            for col in range(ncols):\\n                axs[row, col].clear()\\n                axs[row, col].set_xticks([])\\n                axs[row, col].set_yticks([])\\n                plots.append(axs[row, col].imshow(store[i, (row * ncols) + col]))\\n        return plots\\n\\n    ani = FuncAnimation(\\n        fig,\\n        animate_diff,\\n        fargs=[nsx_gen_store],\\n        interval=200,\\n        blit=False,\\n        repeat=True,\\n        frames=nsx_gen_store.shape[0],\\n    )\\n    plt.close()\\n    if save:\\n        ani.save(save_dir + f\\\"{fn}_w{w}.gif\\\", dpi=100, writer=PillowWriter(fps=5))\\n        print(\\\"saved gif at \\\" + save_dir + f\\\"{fn}_w{w}.gif\\\")\\n    return ani\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.same_channels = in_channels == out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, 3, 1, 1\n",
    "            ),  # 3x3 kernel with stride 1 and padding 1\n",
    "            nn.BatchNorm2d(out_channels),  # Batch normalization\n",
    "            nn.GELU(),  # GELU activation function\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                out_channels, out_channels, 3, 1, 1\n",
    "            ),  # 3x3 kernel with stride 1 and padding 1\n",
    "            nn.BatchNorm2d(out_channels),  # Batch normalization\n",
    "            nn.GELU(),  # GELU activation function\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                shortcut = nn.Conv2d(\n",
    "                    x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0\n",
    "                ).to(x.device)\n",
    "                out = shortcut(x) + x2\n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "    def get_out_channels(self):\n",
    "        return self.conv2[0].out_channels\n",
    "\n",
    "    def set_out_channels(self, out_channels):\n",
    "        self.conv1[0].out_channels = out_channels\n",
    "        self.conv2[0].in_channels = out_channels\n",
    "        self.conv2[0].out_channels = out_channels\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        layers = [\n",
    "            ResidualConvBlock(in_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            nn.MaxPool2d(2),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        \"\"\"This class defines a generic one layer feed-forward neural network for embedding input data of\n",
    "        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def unorm(x):\n",
    "    \"\"\"unity norm. results in range of [0,1]\n",
    "    assumes x has shape (h,w,3)\n",
    "    \"\"\"\n",
    "    xmax = x.max((0, 1))\n",
    "    xmin = x.min((0, 1))\n",
    "    return (x - xmin) / (xmax - xmin)\n",
    "\n",
    "\n",
    "def norm_all(store, n_t, n_s):\n",
    "    \"\"\"runs unity norm on all timesteps of all samples\"\"\"\n",
    "    nstore = np.zeros_like(store)\n",
    "    for t in range(n_t):\n",
    "        for s in range(n_s):\n",
    "            nstore[t, s] = unorm(store[t, s])\n",
    "    return nstore\n",
    "\n",
    "\n",
    "def norm_torch(x_all):\n",
    "    \"\"\"runs unity norm on all timesteps of all samples\n",
    "    input is assumed to be in (bs,3,h,w), the torch image format\"\"\"\n",
    "    x = x_all.cpu().numpy()\n",
    "    xmax = x.max((2, 3))\n",
    "    xmin = x.min((2, 3))\n",
    "    xmax = np.expand_dims(xmax, (2, 3))\n",
    "    xmin = np.expand_dims(xmin, (2, 3))\n",
    "    nstore = (x - xmin) / (xmax - xmin)\n",
    "    return torch.from_numpy(nstore)\n",
    "\n",
    "\n",
    "def gen_tst_context(n_cfeat):\n",
    "    \"\"\"\n",
    "    Generate test context vectors\n",
    "    \"\"\"\n",
    "    vec = torch.tensor(\n",
    "        [\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "        ]\n",
    "    )\n",
    "    return len(vec), vec\n",
    "\n",
    "\n",
    "def plot_grid(x, n_sample, n_rows, save_dir, w):\n",
    "    \"\"\"x: (n_sample, 3, h, w)\"\"\"\n",
    "    ncols = n_sample // n_rows\n",
    "    grid = make_grid(\n",
    "        norm_torch(x), nrow=ncols\n",
    "    )  # curiously, nrow is number of columns.. or number of items in the row.\n",
    "    save_image(grid, save_dir + f\"run_image_w{w}.png\")\n",
    "    print(\"saved image at \" + save_dir + f\"run_image_w{w}.png\")\n",
    "    return grid\n",
    "\n",
    "\n",
    "def plot_sample(x_gen_store, n_sample, nrows, save_dir, fn, w, save=False):\n",
    "    ncols = n_sample // nrows\n",
    "    sx_gen_store = np.moveaxis(\n",
    "        x_gen_store, 2, 4\n",
    "    )  # change to Numpy image format (h,w,channels) vs (channels,h,w)\n",
    "    nsx_gen_store = norm_all(\n",
    "        sx_gen_store, sx_gen_store.shape[0], n_sample\n",
    "    )  # unity norm to put in range [0,1] for np.imshow\n",
    "\n",
    "    # create gif of images evolving over time, based on x_gen_store\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, sharex=True, sharey=True, figsize=(ncols, nrows)\n",
    "    )\n",
    "\n",
    "    def animate_diff(i, store):\n",
    "        print(f\"gif animating frame {i} of {store.shape[0]}\", end=\"\\r\")\n",
    "        plots = []\n",
    "        for row in range(nrows):\n",
    "            for col in range(ncols):\n",
    "                axs[row, col].clear()\n",
    "                axs[row, col].set_xticks([])\n",
    "                axs[row, col].set_yticks([])\n",
    "                plots.append(axs[row, col].imshow(store[i, (row * ncols) + col]))\n",
    "        return plots\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig,\n",
    "        animate_diff,\n",
    "        fargs=[nsx_gen_store],\n",
    "        interval=200,\n",
    "        blit=False,\n",
    "        repeat=True,\n",
    "        frames=nsx_gen_store.shape[0],\n",
    "    )\n",
    "    plt.close()\n",
    "    if save:\n",
    "        ani.save(save_dir + f\"{fn}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "        print(\"saved gif at \" + save_dir + f\"{fn}_w{w}.gif\")\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# | export\\n\\n\\nclass ContextUnet(nn.Module):\\n    def __init__(\\n        self, in_channels, n_feat=256, n_cfeat=10, height=28\\n    ):  # cfeat - context features\\n        super(ContextUnet, self).__init__()\\n        # number of input channels, number of intermediate feature maps and number of classes\\n        self.in_channels = in_channels\\n        self.n_feat = n_feat\\n        self.n_cfeat = n_cfeat\\n        self.h = height  # assume h == w. must be divisible by 4, so 28,24,20,16...\\n        # Initialize the initial convolutional layer\\n        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\\n        # Initialize the down-sampling path of the U-Net with two levels\\n        self.down1 = UnetDown(n_feat, n_feat)  # down1 #[10, 256, 8, 8]\\n        self.down2 = UnetDown(n_feat, 2 * n_feat)  # down2 #[10, 256, 4,  4]\\n        # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\\n        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\\n        # Embed the timestep and context labels with a one-layer fully connected neural network\\n        self.timeembed1 = EmbedFC(1, 2 * n_feat)\\n        self.timeembed2 = EmbedFC(1, 1 * n_feat)\\n        self.contextembed1 = EmbedFC(n_cfeat, 2 * n_feat)\\n        self.contextembed2 = EmbedFC(n_cfeat, 1 * n_feat)\\n        # Initialize the up-sampling path of the U-Net with three levels\\n        self.up0 = nn.Sequential(\\n            nn.ConvTranspose2d(\\n                2 * n_feat, 2 * n_feat, self.h // 4, self.h // 4\\n            ),  # up-sample\\n            nn.GroupNorm(8, 2 * n_feat),  # normalize\\n            nn.ReLU(),\\n        )\\n        self.up1 = UnetUp(4 * n_feat, n_feat)\\n        self.up2 = UnetUp(2 * n_feat, n_feat)\\n        # Initialize the final convolutional layers to map to the same number of channels as the input image\\n        self.out = nn.Sequential(\\n            nn.Conv2d(\\n                2 * n_feat, n_feat, 3, 1, 1\\n            ),  # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\\n            nn.GroupNorm(8, n_feat),  # normalize\\n            nn.ReLU(),\\n            nn.Conv2d(\\n                n_feat, self.in_channels, 3, 1, 1\\n            ),  # map to same number of channels as input\\n        )\\n\\n    def forward(self, x, t, c=None):\\n        \\\"\\\"\\\"\\n        x : (batch, n_feat, h, w) : input image\\n        t : (batch, n_cfeat)      : time step\\n        c : (batch, n_classes)    : context label\\n        \\\"\\\"\\\"\\n        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\\n        # pass the input image through the initial convolutional layer\\n        x = self.init_conv(x)\\n        # print(f'After initial convolution: {x.shape}')\\n        # pass the result through the down-sampling path\\n        down1 = self.down1(x)  # [10, 256, 8, 8]\\n        # print(f'After down1: {down1.shape}')\\n        down2 = self.down2(down1)  # [10, 256, 4, 4]\\n        # print(f'After down2: {down2.shape}')\\n        # convert the feature maps to a vector and apply an activation\\n        hiddenvec = self.to_vec(down2)\\n        # print(f'After converting to vector: {hiddenvec.shape}')\\n        # mask out context if context_mask == 1\\n        if c is None:\\n            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\\n        # embed context and timestep\\n        cemb1 = self.contextembed1(c).view(\\n            -1, self.n_feat * 2, 1, 1\\n        )  # (batch, 2*n_feat, 1,1)\\n        # print(f'After cemb1: {cemb1.shape}')\\n        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\\n        # print(f'After temb1: {temb1.shape}')\\n        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\\n        # print(f'After cemb2: {cemb2.shape}')\\n        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\\n        # print(f'After temb2: {temb2.shape}')\\n        # print(f\\\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\\\")\\n        up1 = self.up0(hiddenvec)\\n        # print(f'After up0: {up1.shape}')\\n        up2 = self.up1(cemb1 * up1 + temb1, down2)  # add and multiply embeddings\\n        # print(f'After up1: {up2.shape}')\\n        up3 = self.up2(cemb2 * up2 + temb2, down1)\\n        # print(f'After up2: {up3.shape}')\\n        out = self.out(torch.cat((up3, x), 1))\\n        # print(f'Final output shape: {out.shape}')\\n        return out\";\n",
       "                var nbb_formatted_code = \"# | export\\n\\n\\nclass ContextUnet(nn.Module):\\n    def __init__(\\n        self, in_channels, n_feat=256, n_cfeat=10, height=28\\n    ):  # cfeat - context features\\n        super(ContextUnet, self).__init__()\\n        # number of input channels, number of intermediate feature maps and number of classes\\n        self.in_channels = in_channels\\n        self.n_feat = n_feat\\n        self.n_cfeat = n_cfeat\\n        self.h = height  # assume h == w. must be divisible by 4, so 28,24,20,16...\\n        # Initialize the initial convolutional layer\\n        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\\n        # Initialize the down-sampling path of the U-Net with two levels\\n        self.down1 = UnetDown(n_feat, n_feat)  # down1 #[10, 256, 8, 8]\\n        self.down2 = UnetDown(n_feat, 2 * n_feat)  # down2 #[10, 256, 4,  4]\\n        # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\\n        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\\n        # Embed the timestep and context labels with a one-layer fully connected neural network\\n        self.timeembed1 = EmbedFC(1, 2 * n_feat)\\n        self.timeembed2 = EmbedFC(1, 1 * n_feat)\\n        self.contextembed1 = EmbedFC(n_cfeat, 2 * n_feat)\\n        self.contextembed2 = EmbedFC(n_cfeat, 1 * n_feat)\\n        # Initialize the up-sampling path of the U-Net with three levels\\n        self.up0 = nn.Sequential(\\n            nn.ConvTranspose2d(\\n                2 * n_feat, 2 * n_feat, self.h // 4, self.h // 4\\n            ),  # up-sample\\n            nn.GroupNorm(8, 2 * n_feat),  # normalize\\n            nn.ReLU(),\\n        )\\n        self.up1 = UnetUp(4 * n_feat, n_feat)\\n        self.up2 = UnetUp(2 * n_feat, n_feat)\\n        # Initialize the final convolutional layers to map to the same number of channels as the input image\\n        self.out = nn.Sequential(\\n            nn.Conv2d(\\n                2 * n_feat, n_feat, 3, 1, 1\\n            ),  # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\\n            nn.GroupNorm(8, n_feat),  # normalize\\n            nn.ReLU(),\\n            nn.Conv2d(\\n                n_feat, self.in_channels, 3, 1, 1\\n            ),  # map to same number of channels as input\\n        )\\n\\n    def forward(self, x, t, c=None):\\n        \\\"\\\"\\\"\\n        x : (batch, n_feat, h, w) : input image\\n        t : (batch, n_cfeat)      : time step\\n        c : (batch, n_classes)    : context label\\n        \\\"\\\"\\\"\\n        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\\n        # pass the input image through the initial convolutional layer\\n        x = self.init_conv(x)\\n        # print(f'After initial convolution: {x.shape}')\\n        # pass the result through the down-sampling path\\n        down1 = self.down1(x)  # [10, 256, 8, 8]\\n        # print(f'After down1: {down1.shape}')\\n        down2 = self.down2(down1)  # [10, 256, 4, 4]\\n        # print(f'After down2: {down2.shape}')\\n        # convert the feature maps to a vector and apply an activation\\n        hiddenvec = self.to_vec(down2)\\n        # print(f'After converting to vector: {hiddenvec.shape}')\\n        # mask out context if context_mask == 1\\n        if c is None:\\n            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\\n        # embed context and timestep\\n        cemb1 = self.contextembed1(c).view(\\n            -1, self.n_feat * 2, 1, 1\\n        )  # (batch, 2*n_feat, 1,1)\\n        # print(f'After cemb1: {cemb1.shape}')\\n        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\\n        # print(f'After temb1: {temb1.shape}')\\n        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\\n        # print(f'After cemb2: {cemb2.shape}')\\n        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\\n        # print(f'After temb2: {temb2.shape}')\\n        # print(f\\\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\\\")\\n        up1 = self.up0(hiddenvec)\\n        # print(f'After up0: {up1.shape}')\\n        up2 = self.up1(cemb1 * up1 + temb1, down2)  # add and multiply embeddings\\n        # print(f'After up1: {up2.shape}')\\n        up3 = self.up2(cemb2 * up2 + temb2, down1)\\n        # print(f'After up2: {up3.shape}')\\n        out = self.out(torch.cat((up3, x), 1))\\n        # print(f'Final output shape: {out.shape}')\\n        return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, n_feat=256, n_cfeat=10, height=28\n",
    "    ):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  # assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)  # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)  # down2 #[10, 256, 4,  4]\n",
    "        # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2 * n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1 * n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2 * n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1 * n_feat)\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * n_feat, 2 * n_feat, self.h // 4, self.h // 4\n",
    "            ),  # up-sample\n",
    "            nn.GroupNorm(8, 2 * n_feat),  # normalize\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                2 * n_feat, n_feat, 3, 1, 1\n",
    "            ),  # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat),  # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                n_feat, self.in_channels, 3, 1, 1\n",
    "            ),  # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # print(f'After initial convolution: {x.shape}')\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)  # [10, 256, 8, 8]\n",
    "        # print(f'After down1: {down1.shape}')\n",
    "        down2 = self.down2(down1)  # [10, 256, 4, 4]\n",
    "        # print(f'After down2: {down2.shape}')\n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        # print(f'After converting to vector: {hiddenvec.shape}')\n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(\n",
    "            -1, self.n_feat * 2, 1, 1\n",
    "        )  # (batch, 2*n_feat, 1,1)\n",
    "        # print(f'After cemb1: {cemb1.shape}')\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        # print(f'After temb1: {temb1.shape}')\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        # print(f'After cemb2: {cemb2.shape}')\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        # print(f'After temb2: {temb2.shape}')\n",
    "        # print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # print(f'After up0: {up1.shape}')\n",
    "        up2 = self.up1(cemb1 * up1 + temb1, down2)  # add and multiply embeddings\n",
    "        # print(f'After up1: {up2.shape}')\n",
    "        up3 = self.up2(cemb2 * up2 + temb2, down1)\n",
    "        # print(f'After up2: {up3.shape}')\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        # print(f'Final output shape: {out.shape}')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# | hide\\nimport nbdev\\n\\nnbdev.nbdev_export()\";\n",
       "                var nbb_formatted_code = \"# | hide\\nimport nbdev\\n\\nnbdev.nbdev_export()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcvp-book",
   "language": "python",
   "name": "mcvp-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
